# -*- coding: utf-8 -*-
"""UniMiB_SHAR_ADL_load_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U1EY6cZsOFERD3Df1HRqjuTq5bDUGH03

#UniMiB_SHAR_ADL_load_dataset.ipynb. 
Loads the A-9 (ADL) portion of the UniMiB dataset from the Internet repository and converts the data into numpy arrays while adhering to the general format of the [Keras MNIST load_data function](https://keras.io/api/datasets/mnist/#load_data-function).

Arguments: tbd
Returns: Tuple of Numpy arrays:   
(x_train, y_train),(x_validation, y_validation)\[optional\],(x_test, y_test) 

* x_train\/validation\/test: containing float64 with shapes (num_samples, 151, {3,4,1})
* y_train\/validation\/test: containing int8 with shapes (num_samples 0-9)

The train/test split is by subject

Example usage:  
x_train, y_train, x_test, y_test = unimib_load_dataset()

Additional References  
 If you use the dataset and/or code, please cite this paper (downloadable from [here](http://www.mdpi.com/2076-3417/7/10/1101/html))

Developed and tested using colab.research.google.com  
To save as .py version use File > Download .py

Author:  Lee B. Hinkle, IMICS Lab, Texas State University, 2021

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.


TODOs:
* Fix document strings
* Assign names to activities instead of numbers
"""

import os
import shutil #https://docs.python.org/3/library/shutil.html
from shutil import unpack_archive # to unzip
#from shutil import make_archive # to create zip for storage
import requests #for downloading zip file
from scipy import io #for loadmat, matlab conversion
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt # for plotting - pandas uses matplotlib
from tabulate import tabulate # for verbose tables
#from tensorflow.keras.utils import to_categorical # for one-hot encoding

#credit https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url
#many other methods I tried failed to download the file properly
from torch.utils.data import Dataset, DataLoader

# scaler
from sklearn.preprocessing import StandardScaler

#data augmentation
import tsaug

#class_dict = {'StandingUpFS':0,'StandingUpFL':1,'Walking':2,'Running':3,'GoingUpS':4,'Jumping':5,'GoingDownS':6,'LyingDownFS':7,'SittingDown':8}

class stock_load_dataset(Dataset):
    def __init__(
            self, 
            verbose = False,
            is_normalize = True,
            # split_subj = dict
            #         (train_subj = [4,5,6,7,8,10,11,12,14,15,19,20,21,22,24,26,27,29],
            #         validation_subj = [1,9,16,23,25,28],
            #         test_subj = [2,3,13,17,18,30]),
            one_hot_encode = False,
            data_mode = 'Train',
            #single_class = False,
            #class_name= 'Walking',
            augment_times = None,
            max_seq_len = 150,
            padding_value: float=-1.0,
        ):
        self.verbose = verbose
        #self.split_subj = split_subj
        self.one_hot_encode = one_hot_encode
        self.data_mode = data_mode
        #self.class_name = class_name
        #self.single_class = single_class
        self.is_normalize = is_normalize
        
        #Convert .mat files to numpy ndarrays
        path_in = './stock_data/'
        file_name = path_in + 'stock.csv'
        #loadmat loads matlab files as dictionary, keys: header, version, globals, data
        stock_data = pd.read_csv(file_name)
        if(self.verbose):
            print(f'stock_data shape is {stock_data.shape}')
            print(stock_data.head())

        if self.is_normalize:
            scaler = StandardScaler()
            scaler.fit(stock_data)
            params = [scaler.mean_, scaler.var_]

        #Normalize and reshape the data

        no = stock_data.shape[0]
        dim = len(stock_data.columns)
        #num_samples = seq_len
        #adl_data = np.reshape(adl_data,(-1,num_samples,3), order='F') #uses Fortran order

        #output = np.empty([no-max_seq_len, max_seq_len, 1 , dim])  # Shape:[no, max_seq_len, dim]
        output = np.empty([no-max_seq_len, dim, 1, max_seq_len])
        output.fill(padding_value)
        time = []

        # For timestep in the dataset
        for i in range(no-max_seq_len):
            # Extract the time-series data with a certain admissionid

            #curr_data = ori_data[ori_data[index] == uniq_id[i]].to_numpy()
            curr_data = stock_data[i:i + max_seq_len]

            # Impute missing data
            #curr_data = imputer(curr_data, impute_vals)

            # Normalize data
            curr_data = scaler.transform(curr_data)
            
            # Extract time and assign to the preprocessed data (Excluding ID)
            curr_no = len(curr_data)
            curr_data = np.transpose(curr_data, (1, 0))
            # Pad data to `max_seq_len`
            if curr_no >= max_seq_len:
                output[i, :, 0, :] = curr_data[:max_seq_len, :]  # Shape: [1, max_seq_len, dim]
                time.append(max_seq_len)
            else:
                output[i, :,0 , :curr_no] = curr_data[:, :]  # Shape: [1, max_seq_len, dim]
                time.append(curr_no)
        print(f'output shape is {output.shape}')

        ii_split = int((no-max_seq_len) * 0.8) # 80% train, 20% test
        self.x_train = output[:ii_split]
        self.x_test = output[ii_split:]

        # set y_train, y_test to 1
        self.y_train = np.ones(self.x_train.shape[0])
        self.y_test = np.ones(self.x_test.shape[0])

        print(f'x_train shape is {self.x_train.shape}, x_test shape is {self.x_test.shape}')
        print(f'y_train shape is {self.y_train.shape}, y_test shape is {self.y_test.shape}')
    
    
    def __len__(self):
        if self.data_mode == 'Train':
            return len(self.y_train)
        else:
            return len(self.y_test)
        
    def __getitem__(self, idx):
        if self.data_mode == 'Train':
            return self.x_train[idx], self.y_train[idx]
        else:
            return self.x_test[idx], self.y_test[idx]
            
    